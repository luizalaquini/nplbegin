{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ProphetNET\n",
    "\n",
    "É um modelo de geração de texto baseado em Transformers que utiliza uma técnica de predição autoregressiva aprimorada chamada autoregressão profética, que se concentra na previsão das próximas palavras mais importantes em uma sequência de texto (Microsoft).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ProphetNetTokenizer, ProphetNetForConditionalGeneration\n",
    "\n",
    "# Load the pre-trained ProphetNet model and tokenizer\n",
    "tokenizer = ProphetNetTokenizer.from_pretrained('microsoft/prophetnet-large-uncased')\n",
    "model = ProphetNetForConditionalGeneration.from_pretrained('microsoft/prophetnet-large-uncased')\n",
    "\n",
    "# Define your input text\n",
    "input_text = \"The future of AI in healthcare is\"\n",
    "\n",
    "# Encode the input text\n",
    "input_tokens = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate predictions (output tokens) from the model\n",
    "output_tokens = model.generate(input_tokens, max_length=50, num_beams=5, early_stopping=True)\n",
    "\n",
    "# Decode the output tokens to a readable string\n",
    "generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformers\n",
    "\n",
    "São modelos de linguagem baseados em Transformers que utilizam uma arquitetura eficiente de memória longa e reduzem a complexidade computacional por meio de um mecanismo de atenção esparsa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ReformerModel, ReformerTokenizer\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"google/reformer-enwik8\"\n",
    "model = ReformerModel.from_pretrained(model_name)\n",
    "tokenizer = ReformerTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Generate text\n",
    "input_text = \"Reformer is known for its efficiency.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "output = model.generate(input_ids)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELECTRA \n",
    "\n",
    "É um modelo pré-treinado baseado em Transformers que propõe uma abordagem eficiente de pré-treinamento de linguagem, onde um discriminador é treinado para distinguir tokens reais de tokens gerados pelo modelo, resultando em um treinamento mais rápido e eficiente (Google)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraTokenizer, ElectraForSequenceClassification, pipeline\n",
    "\n",
    "# Load the pre-trained ELECTRA model and tokenizer\n",
    "model_name = \"google/electra-small-discriminator\"\n",
    "tokenizer = ElectraTokenizer.from_pretrained(model_name)\n",
    "model = ElectraForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Initialize a pipeline for sequence classification\n",
    "nlp = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Define the input text\n",
    "text = \"The movie was fantastic and full of suspense. Definitely a must-watch!\"\n",
    "\n",
    "# Use the pipeline to classify the text\n",
    "classification = nlp(text)\n",
    "\n",
    "# Print the classification result\n",
    "print(\"Classification Result:\", classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CTRL \n",
    "\n",
    "É um modelo de linguagem condicionado baseado em Transformers que gera texto com base em um contexto inicial específico fornecido pelo usuário, permitindo a geração controlada e direcionada de texto (OpenAI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may run slow\n",
    "from transformers import CTRLTokenizer, CTRLLMHeadModel\n",
    "\n",
    "# Load the pre-trained CTRL model and tokenizer\n",
    "model_name = 'ctrl'\n",
    "tokenizer = CTRLTokenizer.from_pretrained(model_name)\n",
    "model = CTRLLMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Define a control code and input text. For example, 'Links' control code generates text in the style of a Wikipedia article.\n",
    "control_code = \"Links\"\n",
    "input_text = control_code + \" The Great Pyramid of Giza\"\n",
    "input_tokens = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text using the model\n",
    "output_tokens = model.generate(input_tokens, max_length=100, temperature=0.7)\n",
    "\n",
    "# Decode the generated tokens to a readable string\n",
    "generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pegasus\n",
    "\n",
    "É um modelo de geração de texto baseado em Transformers que utiliza uma arquitetura de codificador-decodificador para gerar resumos de texto de alta qualidade, sendo especialmente eficaz para tarefas de geração de texto abstrato (Google)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "\n",
    "# Load the pre-trained PEGASUS model and tokenizer\n",
    "model_name = 'google/pegasus-xsum'\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Define your input text that you want to summarize\n",
    "input_text = \"\"\"The COVID-19 pandemic has led to a dramatic loss of human life worldwide and presents an unprecedented challenge to public health, food systems, and the world of work. The economic and social disruption caused by the pandemic is devastating: tens of millions of people are at risk of falling into extreme poverty, while the number of undernourished people, currently estimated at nearly 690 million, could increase by up to 132 million by the end of the year.\"\"\"\n",
    "\n",
    "# Encode the input text\n",
    "input_tokens = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate the summary\n",
    "summary_ids = model.generate(input_tokens, max_length=45, num_beams=4, length_penalty=2.0, early_stopping=True)\n",
    "\n",
    "# Decode the summary\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Summary:\", summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ERNIE\n",
    "\n",
    "É um modelo de linguagem pré-treinado baseado em Transformers, desenvolvido pela Baidu, que incorpora conhecimento de entidades e relações em sua representação, permitindo uma melhor compreensão e geração de texto em contextos específicos (Baidu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained ERNIE model and tokenizer\n",
    "model_name = 'nghuyong/ernie-2.0-en'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Encode the sentence\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# Interpret the model output (for example, by applying a softmax function to obtain probabilities for each class)\n",
    "probabilities = torch.softmax(logits, dim=1)\n",
    "\n",
    "print(\"Probabilities:\", probabilities)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
