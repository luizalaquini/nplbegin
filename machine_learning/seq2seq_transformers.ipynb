{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq\n",
    "\n",
    "Os modelos Sequence-to-Sequence (Seq2Seq) são um pilar no campo do processamento de linguagem natural, especialmente para tarefas que envolvem a geração de sequências a partir de outras sequências, como tradução automática, resumo de texto e resposta a perguntas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo em uma tarefa de tradução:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get data\n",
    "#!wget http://www.manythings.org/anki/fra-eng.zip\n",
    "#!unzip fra-eng.zip\n",
    "\n",
    "# Libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "with open('fra.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split(\"\n",
    "\")\n",
    "\n",
    "# Extract sentence pairs\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "\n",
    "# Use \"\\t\" as the start sequence character and \"\\n\" as the end sequence character\n",
    "for line in lines[:10000]:  # Using the first 10,000 sentence pairs for simplicity\n",
    "    input_text, target_text, _ = line.split(\"\\t\")\n",
    "    target_text = \"\\t\" + target_text + \"\\n\"  \n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "\n",
    "# Tokenization\n",
    "tokenizer_in = Tokenizer(char_level=True)\n",
    "tokenizer_in.fit_on_texts(input_texts)\n",
    "encoder_input_data = tokenizer_in.texts_to_sequences(input_texts)\n",
    "encoder_input_data = pad_sequences(encoder_input_data, padding='post')\n",
    "\n",
    "tokenizer_out = Tokenizer(char_level=True)\n",
    "tokenizer_out.fit_on_texts(target_texts)\n",
    "decoder_input_data = tokenizer_out.texts_to_sequences(target_texts)\n",
    "decoder_input_data = pad_sequences(decoder_input_data, padding='post')\n",
    "decoder_target_data = np.roll(decoder_input_data, -1, axis=1)  # Shift decoder input for target data\n",
    "\n",
    "# Convert to one-hot encoding\n",
    "encoder_input_data = to_categorical(encoder_input_data)\n",
    "decoder_input_data = to_categorical(decoder_input_data)\n",
    "decoder_target_data = to_categorical(decoder_target_data)\n",
    "\n",
    "# Set model parameters\n",
    "num_encoder_tokens = encoder_input_data.shape[2]\n",
    "num_decoder_tokens = decoder_input_data.shape[2]\n",
    "max_encoder_seq_length = encoder_input_data.shape[1]\n",
    "max_decoder_seq_length = decoder_input_data.shape[1]\n",
    "\n",
    "# Split data\n",
    "encoder_input_train, encoder_input_val, decoder_input_train, decoder_input_val, decoder_target_train, decoder_target_val = train_test_split(encoder_input_data, decoder_input_data, decoder_target_data, test_size=0.2)\n",
    "\n",
    "# Define the encoder\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(256, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Define the decoder\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the Seq2Seq model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training\n",
    "model.fit([encoder_input_train, decoder_input_train], decoder_target_train,\n",
    "          batch_size=64,\n",
    "          epochs=30,  # Increase epochs for better results\n",
    "          validation_data=([encoder_input_val, decoder_input_val], decoder_target_val))\n",
    "\n",
    "# Define the encoder model for inference\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Define the decoder model for inference\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Create a function to decode sequences\n",
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, tokenizer_out.word_index['\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = tokenizer_out.index_word.get(sampled_token_index, '')\n",
    "\n",
    "        if sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length:\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            decoded_sentence += sampled_char\n",
    "\n",
    "            target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "            target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "            states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "# Predict\n",
    "for seq_index in range(10):\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Mechanisms\n",
    "\n",
    "Os mecanismos de atenção evoluíram a PNL, oferecendo uma maneira mais eficiente e eficaz para os modelos processarem e relacionarem diferentes partes de uma sequência. Originalmente introduzidos no contexto da tradução automática neural, os mecanismos de atenção são agora onipresentes em várias tarefas de modelagem de sequência."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo de implementação em uma tarefa de tradução:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_encoder_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Sample encoder-decoder model with attention\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m input_seq \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[43mnum_encoder_tokens\u001b[49m))\n\u001b[0;32m      7\u001b[0m encoder \u001b[38;5;241m=\u001b[39m LSTM(latent_dim, return_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m encoder_outputs, state_h, state_c \u001b[38;5;241m=\u001b[39m encoder(input_seq)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_encoder_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Concatenate, Attention\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Sample encoder-decoder model with attention\n",
    "input_seq = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(input_seq)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "attention = Attention()\n",
    "attention_out = attention([decoder_outputs, encoder_outputs])\n",
    "decoder_concat = Concatenate(axis=-1)([decoder_outputs, attention_out])\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_concat)\n",
    "\n",
    "model = Model([input_seq, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Model compilation, training, and evaluation code would follow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "Os transformadores representam um grande avanço nas arquiteturas de redes neurais; eles melhoraram significativamente o desempenho por meio do uso de blocos de atenção em conjunto com redes neurais de feed-forward simples e contornaram os problemas que acompanham as unidades RNN. Eles são a espinha dorsal dos modelos modernos de grandes linguagens e entrarei em muitos detalhes sobre eles aqui (e também, mais especificamente para LLMs em uma seção posterior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load a smaller subset of the IMDB dataset for quick training\n",
    "dataset = load_dataset('imdb', split={'train': 'train[:10%]', 'test': 'test[:10%]'})\n",
    "\n",
    "# 2. Load the DistilBert tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Function to tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "# Tokenize the entire dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 3. Load the DistilBert model for sequence classification\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "# 4. Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',         # output directory for model checkpoints\n",
    "    num_train_epochs=2,             # number of training epochs for demonstration\n",
    "    per_device_train_batch_size=8,  # batch size for training\n",
    "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
    "    logging_dir='./logs',           # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",    # evaluate at the end of each epoch\n",
    ")\n",
    "\n",
    "# Function to compute accuracy\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {'accuracy': accuracy_score(labels, predictions)}\n",
    "\n",
    "# 5. Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "# 6. Perform inference on a new sentence\n",
    "new_sentences = [\"This movie is fantastic!\", \"I did not like this movie at all.\"]\n",
    "new_inputs = tokenizer(new_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "predictions = model(**new_inputs).logits\n",
    "predicted_classes = np.argmax(predictions.detach().numpy(), axis=1)\n",
    "print(\"Predictions:\", predicted_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
