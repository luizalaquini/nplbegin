{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Processo de reduzir uma palavra para o seu radical ('stem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['program', 'program', 'program', 'program', 'program']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "words = [\"program\", \"programs\", \"programer\", \"programing\", \"programers\"]\n",
    "stems = [ps.stem(word) for word in words]\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "Processo de retornar a 'base' de uma palavra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['be', 'be', 'be', 'good', 'better']\n",
      "['am', 'are', 'is', 'good', 'well']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [\"am\", \"are\", \"is\", \"good\", \"better\"]\n",
    "\n",
    "''' :param pos: The Part Of Speech tag. Valid options are `\"n\"` for nouns,\n",
    "              `\"v\"` for verbs, `\"a\"` for adjectives, `\"r\"` for adverbs and `\"s\"`\n",
    "              for satellite adjectives.'''\n",
    "lemmas_verbs = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "lemmas_adverbs = [lemmatizer.lemmatize(word, pos='r') for word in words]\n",
    "\n",
    "print(lemmas_verbs)\n",
    "print(lemmas_adverbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging\n",
    "\n",
    "Processo de classificar gramaticalmente as palavras. OBS: Uma dificuldade é lidar com as palavras que podem ter mais de uma classificação gramatical.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Natural', 'ADJ'), ('language', 'NOUN'), ('processing', 'NOUN'), ('empowers', 'VERB'), ('computational', 'ADJ'), ('systems', 'NOUN'), ('to', 'PRT'), ('understand', 'VERB'), ('human', 'ADJ'), ('language', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#import nltk\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('universal_tagset')\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "sentence = \"Natural language processing empowers computational systems to understand human language.\"\n",
    "tokens = word_tokenize(sentence)\n",
    "pos_tags = pos_tag(tokens, tagset='universal')\n",
    "\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition (NER)\n",
    "\n",
    "Reconhecimento do contexto das palavras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing\n",
    "\n",
    "Busca compreender como as partes de uma frase (substantivos, verbos, adjetivos) são organizadas para transmitir significado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Constituency Parsing** (*Parsing* de constituintes): identifica os constituintes (frases nominais, sintagmas verbais, etc.) de uma frase e os organiza em uma árvore de análise, que reflete a sintaxe da frase de acordo com uma gramática formal. A árvore mostra como diferentes partes da frase se agrupam para formar frases e orações.\n",
    "\n",
    "Legenda:\n",
    "- S: 'Sentence', a raiz da árvore;\n",
    "- NP: 'Noun Phrase', frase que funciona como substantivo;\n",
    "- VP: 'Verb Phrase', frase que funciona como verbo;\n",
    "- NNP: 'Proper Noun', singular;\n",
    "- VBD: 'Verb', pretérito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "'''nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')'''\n",
    "\n",
    "from nltk import pos_tag, word_tokenize, Tree\n",
    "\n",
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "tokens = word_tokenize(sentence)\n",
    "tags = pos_tag(tokens)\n",
    "\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\" # onde DT é o determinante, JJ é o adjetivo e NN é o substantivo \n",
    "'''NP:\n",
    "      {<DT|JJ>}          # chunk determiners and adjectives\n",
    "      }<[\\.VI].*>+{      # strip any tag beginning with V, I, or .\n",
    "      <.*>}{<DT>         # split a chunk at a determiner\n",
    "      <DT|JJ>{}<NN.*>    # merge chunk ending with det/adj\n",
    "                          # with one starting with a noun'''\n",
    "\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(tags)\n",
    "result.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dependency Parsing** (*Parsing* de dependências): estabelece uma árvore de dependências onde os nós são palavras e as arestas representam dependências entre eles. Estas são as relações gramaticais (como sujeito, objeto, modificador) entre palavras, indicando como cada palavra depende ou está conectada a outras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Role Labeling (SRL)\n",
    "\n",
    "Identificar \"quem fez o que pra quem\", em outras palavras, quem é o sujeito, quem é o agente e quem é o predicado (e outros)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coheference Resolution \n",
    "\n",
    "Tarefa de encontrar todas as expressões que se referem à mesma entidade em um texto. É crucial para tarefas como resumo de documentos e resposta a perguntas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation Extraction \n",
    "\n",
    "Identificar e classificar relações semânticas entre entidades dentro de um texto. Em outras palavras, procurar duas coisas que estão relacionadas de alguma forma (por exemplo, membros da família)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record Linkage\n",
    "\n",
    "Tarefa de vincular registros de diferentes bases de dados que se referem a uma mesma entidade.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por exemplo: você poderia ter um banco de dados gigante de todas as empresas no Canadá – ele realmente existe e é chamado de Registro de Negócios. Freqüentemente, há duplicatas neste conjunto de dados devido a vários registros feitos por diferentes membros da mesma empresa; encontrar essas correspondências seria um exemplo de problema de ligação de registros."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
