{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpora\n",
    "\n",
    "Conjunto de dados de texto (matriz de palavras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\luiza\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Example of loading a popular dataset using NLTK\n",
    "\n",
    "#import nltk\n",
    "#nltk.download('brown')\n",
    "\n",
    "from nltk.corpus import brown\n",
    "print(brown.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Separação de 'tokens'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\luiza\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', 'with', 'Python', 'is', 'super', 'awesome']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Example of word tokenization using NLTK\n",
    "\n",
    "'''import nltk\n",
    "nltk.download('punkt')'''\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Natural Language Processing with Python is super awesome\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "Processo de criar vetores de distâncias entre palavras. Um 'embeeding' é uma representação de uma palavra em forma de vetor criado por um modelo de deep learning para fins de pesquisas de similaridade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.3622725e-04  2.3643136e-04  5.1033497e-03  9.0092728e-03\n",
      " -9.3029495e-03 -7.1168090e-03  6.4588725e-03  8.9729885e-03\n",
      " -5.0154282e-03 -3.7633716e-03  7.3805046e-03 -1.5334714e-03\n",
      " -4.5366134e-03  6.5540518e-03 -4.8601604e-03 -1.8160177e-03\n",
      "  2.8765798e-03  9.9187379e-04 -8.2852151e-03 -9.4488179e-03\n",
      "  7.3117660e-03  5.0702621e-03  6.7576934e-03  7.6286553e-04\n",
      "  6.3508903e-03 -3.4053659e-03 -9.4640139e-04  5.7685734e-03\n",
      " -7.5216377e-03 -3.9361035e-03 -7.5115822e-03 -9.3004224e-04\n",
      "  9.5381187e-03 -7.3191668e-03 -2.3337686e-03 -1.9377411e-03\n",
      "  8.0774371e-03 -5.9308959e-03  4.5162440e-05 -4.7537340e-03\n",
      " -9.6035507e-03  5.0072931e-03 -8.7595852e-03 -4.3918253e-03\n",
      " -3.5099984e-05 -2.9618145e-04 -7.6612402e-03  9.6147433e-03\n",
      "  4.9820580e-03  9.2331432e-03 -8.1579173e-03  4.4957981e-03\n",
      " -4.1370760e-03  8.2453608e-04  8.4986202e-03 -4.4621765e-03\n",
      "  4.5175003e-03 -6.7869602e-03 -3.5484887e-03  9.3985079e-03\n",
      " -1.5776526e-03  3.2137157e-04 -4.1406299e-03 -7.6826881e-03\n",
      " -1.5080082e-03  2.4697948e-03 -8.8802696e-04  5.5336617e-03\n",
      " -2.7429771e-03  2.2600652e-03  5.4557943e-03  8.3459532e-03\n",
      " -1.4537406e-03 -9.2081428e-03  4.3705525e-03  5.7178497e-04\n",
      "  7.4419081e-03 -8.1328274e-04 -2.6384138e-03 -8.7530091e-03\n",
      " -8.5655687e-04  2.8265631e-03  5.4014288e-03  7.0526563e-03\n",
      " -5.7031214e-03  1.8588197e-03  6.0888636e-03 -4.7980510e-03\n",
      " -3.1072604e-03  6.7976294e-03  1.6314756e-03  1.8991709e-04\n",
      "  3.4736372e-03  2.1777749e-04  9.6188262e-03  5.0606038e-03\n",
      " -8.9173904e-03 -7.0415605e-03  9.0145587e-04  6.3925339e-03]\n"
     ]
    }
   ],
   "source": [
    "# Example of generating word embeddings using gensim's Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Define training data\n",
    "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "              ['this', 'is', 'the', 'second', 'sentence'],\n",
    "              ['yet', 'another', 'sentence'],\n",
    "              ['one', 'more', 'sentence'],\n",
    "              ['and', 'the', 'final', 'sentence']]\n",
    "\n",
    "# Train a Word2Vec model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "\n",
    "# Get an embedding for a word\n",
    "word_embedding = model.wv['sentence']\n",
    "print(word_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words (BoW)\n",
    "\n",
    "Matriz de frequências. Eficaz em classificação de documentos e filtragem de spam. Inadequado para a compreensão de nuances linguísticas como sintaxe e semântica (pois não capta a ordem das palavras). Pode levar a uma alta dimensionalidade se o vocabulário for grande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   are  beautiful  day  everything  hello  how  is  it  not  only  the  thing  \\\n",
      "0    1          0    0           0      1    1   0   0    0     0    0      0   \n",
      "1    0          0    0           1      0    0   1   1    1     1    1      1   \n",
      "2    0          1    1           0      0    0   1   0    0     0    0      0   \n",
      "\n",
      "   today  winning  you  \n",
      "0      0        0    1  \n",
      "1      0        1    0  \n",
      "2      1        0    0  \n"
     ]
    }
   ],
   "source": [
    "# Example of creating a Bag of Words model using Python's CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"Winning is not everything, it's the only thing.\",\n",
    "    \"Today is a beautiful day.\"\n",
    "]\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the model and transform the documents\n",
    "bow_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get the feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(bow_matrix.toarray(), columns=feature_names)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "Matriz de importâncias - mitiga palavras genéricas. Eficaz em extração de palavras-chave, modelagem de tópicos e muitos tipos de classificação de texto, como filtragem de e-mails de spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       blue    bright       can        in        is       see   shining  \\\n",
      "0  0.659191  0.000000  0.000000  0.000000  0.420753  0.000000  0.000000   \n",
      "1  0.000000  0.404129  0.000000  0.000000  0.404129  0.000000  0.000000   \n",
      "2  0.000000  0.321846  0.000000  0.504235  0.321846  0.000000  0.000000   \n",
      "3  0.000000  0.239102  0.374599  0.000000  0.000000  0.374599  0.374599   \n",
      "\n",
      "        sky       sun       the     today        we  \n",
      "0  0.519714  0.000000  0.343993  0.000000  0.000000  \n",
      "1  0.000000  0.404129  0.330402  0.633146  0.000000  \n",
      "2  0.397544  0.321846  0.526261  0.000000  0.000000  \n",
      "3  0.000000  0.478204  0.390963  0.000000  0.374599  \n"
     ]
    }
   ],
   "source": [
    "# Example of calculating TF-IDF using scikit-learn's TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"The sky is blue.\",\n",
    "    \"The sun is bright today.\",\n",
    "    \"The sun in the sky is bright.\",\n",
    "    \"We can see the shining sun, the bright sun.\"\n",
    "]\n",
    "\n",
    "# Create a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the model and transform the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get the feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams\n",
    "\n",
    "Sequências de 'n' itens de uma determinada amostra de texto ou fala. Usado para modelar a probabilidade de cada item em uma sequência, com base na ocorrência de itens anteriores. Ferramenta para capturar contexto.\n",
    "\n",
    "- Unigramas (1 grama): Cada item é considerado isoladamente (ex: \"o\", \"gato\").\n",
    "- Bigramas (2 gramas): Sequências de dois itens (ex: \"o gato\", \"gato sentado\").\n",
    "- Trigramas (3 gramas): Sequências de três itens (ex: “o gato sentou”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'need'), ('need', 'to'), ('to', 'write'), ('write', 'a'), ('a', 'sentence'), ('sentence', 'with'), ('with', 'some'), ('some', 'words')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import bigrams\n",
    "\n",
    "text = \"I need to write a sentence with some words\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "bigrams_list = list(bigrams(tokens))\n",
    "\n",
    "print(bigrams_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
